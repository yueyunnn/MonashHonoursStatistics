---
title: "Linear Mixed Models (LMMs) - Part 2"
author: "Joshua F. Wiley"
date: "`r Sys.Date()`"
output: 
  tufte::tufte_html: 
    toc: true
    number_sections: true
---

Download the raw `R` markdown code here
[https://jwiley.github.io/MonashHonoursStatistics/LMM2.rmd](https://jwiley.github.io/MonashHonoursStatistics/LMM2.rmd).
These are the `R` packages we will use.

```{r setup}
options(digits = 2)

## new packages are lme4, lmerTest, and multilevelTools

library(data.table)
library(JWileymisc)
library(lme4)
library(lmerTest)
library(multilevelTools)
library(visreg)
library(ggplot2)
library(ggpubr)

```


```{r}

#### 1. Setup (You Try It) ####

## to start with, we will load a package for data management
## and load a package for fitting Linear Mixed effects Models (LMMs)

## recall that loading a package is like opening an app
## and you need to repeat this process each time you start up R
## if everything installed successfully already
## this code should work and return no errors
## if this does not work, try to install it first
## by uncommenting the install packages code

# install.packages("data.table", dependencies = TRUE)
# install.packages("lme4", dependencies = TRUE)
# install.packages("ggplot2", dependencies = TRUE)
library(data.table)
library(lme4)
library(ggplot2)

## load the data (note this is already an R dataset)
## so we use a new function readRDS()
## make sure that the data file is located in
## your project directory
d <- readRDS("aces_daily_sim_processed.RDS")


## see the names of the variables in the dataset
names(d)


#### 2. Calculating Between & Within Effects (Demonstration)  ####

## examine the distribution of stress in two participants
## you can see how the means seem to differ but also each participant
## has variation within them in their level of stress
ggplot(d[UserID %in% c(1, 2)], aes(STRESS, fill = UserID)) +
  geom_density()

## calculate the individual mean stress BY participant ID
d[, MeanStress := mean(STRESS, na.rm = TRUE), by = UserID]

## calculate the deviation stress scores
## by taking the difference between observed stress scores and the
## indidivudal means
## note that we do not need to specify BY participant ID because the
## individual means are already repeated on each row of the dataset
d[, DeviationStress := STRESS - MeanStress]

## now look at a few rows of the data to check what happened
d[UserID == 1, .(UserID, STRESS, MeanStress, DeviationStress)]

## look at the mean deviation stress (this should basically be 0)
## representing that it is a within only variable
## note that R may use scientific notation:
## 1.4e4 = 1, then move decimal four spots to the right = 14000
## 1.4e-4 = 1, then move decimal four spots to the left = .00014
mean(d$DeviationStress, na.rm=TRUE)


## now we can estimate LMMs

## first we only use STRESS, which combines both between & within effects
summary(lmer(PosAff ~ STRESS + (1 | UserID), data = d))

## next we use our new mean and deviation stress variables
## to separate the between and within effects of stress
summary(lmer(PosAff ~ MeanStress + DeviationStress +
               (1 | UserID), data = d))


#### 3. Calculating Between & Within Effects (You Try It) ####

## in pairs or small groups, pick one of the other variables
## in the dataset (not STRESS, not PosAff) that is repeatedly measured
## calculate individual means and deviations from the means
## then fit a model predicting positive affect, first from the overall
## score and then from the mean and deviation scores.

## if you need a refresher on what variables are available
## take a look at the table in the slides.

## calculate individual means by ID
d[,  := mean( , na.rm = TRUE), by = UserID]

## calculate the deviation scores
d[,  :=  - ]

## fit a linear mixed model using the original variable to predict positive affect
summary(lmer(PosAff ~      + (1 | UserID), data = d))


## fit a linear mixed model using the mean and deviation
## variables to predict positive affect
summary(lmer(PosAff ~      + (1 | UserID), data = d))




#### 4. Random Slopes (Demonstration)  ####

## random intercept and fixed effects only
m1a <- lmer(PosAff ~ MeanStress + DeviationStress +
              (1 | UserID),
            data = d)

## random intercept, random slope, and fixed effects
m1b <- lmer(PosAff ~ MeanStress + DeviationStress +
              (1 | UserID) + (0 + DeviationStress | UserID),
            data = d)

## correlated random intercept and random slope, and fixed effects
m1c <- lmer(PosAff ~ MeanStress + DeviationStress +
              (1 + DeviationStress | UserID),
            data = d)

## generate summaries of the models and compare
## note the standard errors in particular
summary(m1a)
summary(m1b)
summary(m1c)


## does adding the random slope improve model fit?
anova(m1a, m1b, test = "LRT")

## does allow the random intercept and slope to correlate improve model fit?
anova(m1b, m1c, test = "LRT")

## overall does the model with correlated random intercept and slope
## fit better than a random intercept only model
## simultaneously tests 2 parameters: slope variance + 1 correlation
anova(m1a, m1c, test = "LRT")


## Convergence Issue Example

## Example of a model with convergence & fit issues
## note the "singular fit" and convergence warning
summary(lmer(PosAff ~ STRESS + SOLs + NegAff + WASONs +
              (1 + STRESS + SOLs + NegAff + WASONs | UserID),
            data = d))

## this is an example where we might simplify the structure to
## aid convergence and estimation
## note that in the above model with poor convergence
## the SDs for SOLs and WASONs are very small
## we could consider dropping these random slopes
## and just keep as fixed effects
summary(lmer(PosAff ~ STRESS + SOLs + NegAff + WASONs +
               (1 + STRESS + NegAff | UserID),
            data = d))



#### 5. Random Slopes (You Try It)  ####

## in pairs or small groups, use the same variable you chose
## earlier to create individual means and deviations from the means
## use these variables to complete the models below and discuss
## their interpretation amongst yourselves

## random intercept and fixed effects only
m2a <- lmer(PosAff ~    +     +
              (1 | UserID),
            data = d)

## random intercept, random slope, and fixed effects
m2b <- lmer(PosAff ~    +     +
              (1 | UserID) + (0 +  | UserID),
            data = d)

## correlated random intercept and random slope, and fixed effects
m2c <- lmer(PosAff ~      +      +
              (1 +      | UserID),
            data = d)

## generate summaries of the models and compare
## what happens to the standard errors in the fixed
## only vs fixed + random slope models?
summary(  )

## Use the anova() function to answer these questions
## for YOUR variable

## does adding the random slope improve model fit?
anova(    ,     , test = "LRT")

## does allow the random intercept and slope to correlate improve model fit?


## overall does the model with correlated random intercept and slope
## fit better than a random intercept only model?

```


# Synthesis

```{r}

#### 1. Setup (You Try It) ####

## to start with, we will load a package for data management
## and load a package for fitting Linear Mixed effects Models (LMMs)

## recall that loading a package is like opening an app
## and you need to repeat this process each time you start up R
## if everything installed successfully already
## this code should work and return no errors
## if this does not work, try to install it first
## by uncommenting the install packages code

# install.packages("data.table", dependencies = TRUE)
# install.packages("lme4", dependencies = TRUE)
# install.packages("ggplot2", dependencies = TRUE)
library(data.table)
library(lme4)
library(ggplot2)

## load the data (note this is already an R dataset)
## so we use a new function readRDS()
## make sure that the data file is located in
## your project directory
d <- readRDS("aces_daily_sim_processed.RDS")

## see the dimenions of the data (rows   columns)
## should be: 6927   56
dim(d)

## see the names of the variables in the dataset
names(d)

#### 2. Individual, Random, Fixed Effects (Demonstration) ####

## individual
## fit a linear regression of PosAff on STRESS and have R
## save the intercept and slope and do this BY UserID
m.individual <- d[, as.list(coef(lm(PosAff ~ STRESS))), by = UserID]

## histogram of individual slopes
## note a couple of extremes
ggplot(m.individual, aes(STRESS)) + geom_histogram()

## random (LMM)
m.random <- lmer(PosAff ~ STRESS + (1 + STRESS | UserID), data = d)

## get Best Linear Unbiased Predictions (BLUPs) for the individual effects
## from the random effect model (i.e., the LMM estimates of individual effects)
m.ind.random <- ranef(m.random)$UserID

## histogram of BLUPs for the slopes
## note that these are deviations from the fixed effects (the means)
## so mean = 0
ggplot(m.ind.random, aes(STRESS)) + geom_histogram()

## summarise the model
summary(m.random)

## to make it more comparable to individual estimates, add the fixed effects
ggplot(m.ind.random, aes(STRESS - .160623)) + geom_histogram()


## fixed
m.fixed <- lm(PosAff ~ STRESS, data = d)
summary(m.fixed)


## histogram of individual slopes
## with means from different approaches
ggplot(m.individual, aes(STRESS)) + geom_histogram() +
  geom_vline(xintercept = mean(m.individual$STRESS), colour = "black", size = 1) +
  geom_vline(xintercept = -.160623, colour = "blue", size = 1) +
  geom_vline(xintercept = -.183232, colour = "yellow", size = 1)



## examine descriptive statistics from the individual regressions
mean(m.individual[["(Intercept)"]])
sd(m.individual[["(Intercept)"]])

mean(m.individual[["STRESS"]])
sd(m.individual[["STRESS"]])

cor(m.individual[["(Intercept)"]], m.individual[["STRESS"]])

## compare above with the summary from LMM
summary(m.random)



#### 3. Interpreting LMMs (Demonstration) ####

## basic LMM with random intercept and slope (correlated)
m.random <- lmer(PosAff ~ STRESS + (1 + STRESS | UserID), data = d)
summary(m.random)

confint(m.random) ## profile confidence intervals slow here
confint(m.random, method = "Wald") ## Wald faster if CIs for random effects not needed

## ## Sample Write up

## A total of 6,399 observations from 191 unique people were included
## in the analyses. Overall, when people were not experiencing stress, the model
## estimated their positive affect as b [95% CI] = 3.04 [2.93, 3.16].
## However, the random effects revealed substantial differences between
## individual (Random Effect SD = 0.81) such that although the average was 3.04, most people
## fell between 2.23 and 3.86 (i.e., 3.044085 +/- .81104).
## Examining the association of stress with positive affect, on average
## a one unit higher perceived stress score was associated with
## a -0.16 [-0.17, -0.15] difference in positive affect. Again, there were
## differences between individuals (Random effect SD = 0.08) with most people estimated
## to have an association between -0.24 and -0.08 (i.e., -0.160623 +/- 0.07855).
## The random intercept and stress slope were correlated at -0.64 indicating that
## people who had an above average intercept also tended to have a more negative
## association between stress and positive affect. This may indicate that people who are
## generally happier are more sensitive to the effects of stress on mood or
## conversely that people who are generally unhappy simply have less room to worsen
## in response to stress (a floor effect).

## we can check some of the ranges of random effects
## to get random effects with the fixed effects already added (i.e., NOT deviations)
## use coef() instead of ranef()
m.blups <- coef(m.random)$UserID
View(m.blups)

## summarise BLUPs for the random intercept
summary(m.blups[["(Intercept)"]])

## summarise BLUPs for the random slope
summary(m.blups[["STRESS"]])

## if we wanted, instead of using mean +/- SD to give a range of the estimates for
## the random intercept and slope, we could use the 25th and 75th percentiles
## (i.e., 1st and 3rd quartiles of the BLUPs)

## we also can visualise the correlation between the BLUPs for
## the random intercept and slope
ggplot(m.blups, aes(`(Intercept)`, STRESS)) +
  geom_point()

## looking at this we see one person actually has a positive slope
## suggesting that for higher stress they actually have higher positive affect
## that BLUP also appears to be a bit of an outlier.
## its worth investigating such cases a bit to try to understand why
## first, find the case -- #123
subset(m.blups, STRESS > 0)

## now lets investigate the raw data
## use d[UserID == 123] to subset data to only rows where UserID == 123
## make a scatter plot of stress and pos affect for this person
## and no clear signs of outliers or other issues and there
## is a positive association, so may not worry about it, although unusual
ggplot(d[UserID == 123], aes(STRESS, PosAff)) +
  geom_point()

``` 


# Summary Table

Here is a little summary of some of the functions used in this
topic. 

| Function       | What it does                                 |
|----------------|----------------------------------------------|
| `aggr()`     | Create  |
| `marginplot()` | Create  | 

